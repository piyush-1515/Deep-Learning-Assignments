{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import initializers\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data, normalizing and flattening it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading tand Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11385 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n",
      "Found 3795 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train = image_dataset_from_directory(\n",
    "    'Group_24/train/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "val = image_dataset_from_directory(\n",
    "    'Group_24/val/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "test = image_dataset_from_directory(\n",
    "    'Group_24/test/',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=1,\n",
    "    image_size=(28, 28),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "def normalize(image,label):\n",
    "    image = tf.cast(image/255. ,tf.float32)\n",
    "    return image, label\n",
    "\n",
    "train = train.map(normalize)\n",
    "val = val.map(normalize)\n",
    "test = test.map(normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flattening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in train:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "train_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing validation tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in val:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "val_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing testing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataset and reshape each image tensor\n",
    "image_tensors = []\n",
    "label_tensors = []\n",
    "for image, labels in test:\n",
    "    num_images = image.shape[0]\n",
    "    image_vectors = tf.reshape(image, [num_images, -1])\n",
    "    image_tensors.append(image_vectors)\n",
    "    label_tensors.append(labels)\n",
    "\n",
    "# Concatenate the image tensors into a single tensor\n",
    "test_vectors = [tf.concat(image_tensors, axis=0), tf.concat(label_tensors, axis=0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensions to 32, 64, 128 and 256 dimensions through PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = tf.reduce_mean(image_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors[0] -= mean\n",
    "val_vectors[0] -= mean\n",
    "test_vectors[0] -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = [32,64,128,256]\n",
    "\n",
    "def get_reduced_representation(dim, vec):\n",
    "    # Calculate the covariance matrix\n",
    "    cov = tf.matmul(vec[0], vec[0], transpose_a=True)# / tf.cast(tf.shape(vec[0])[0], tf.float32)\n",
    "\n",
    "    # Perform eigenvalue decomposition on the covariance matrix\n",
    "    eigenvalues, eigenvectors = tf.linalg.eigh(cov)\n",
    "\n",
    "    # Sort the eigenvectors by eigenvalues in descending order\n",
    "    sorted_idx = tf.argsort(eigenvalues, direction='DESCENDING')\n",
    "    eigenvectors = tf.gather(eigenvectors, sorted_idx, axis=1)\n",
    "\n",
    "    # Keep only the top k eigenvectors\n",
    "    top_k_eigenvectors = tf.slice(eigenvectors, [0, 0], [-1, num_components[dim]])\n",
    "\n",
    "    # Project the data onto the new basis\n",
    "    reduced_rep = tf.matmul(vec[0], top_k_eigenvectors)\n",
    "    return reduced_rep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hidden = 3\n",
    "model_arch = [\n",
    "    # [4, 4, 4],\n",
    "    # [4, 8, 16],\n",
    "    # [16, 8, 4],\n",
    "    # [8, 8, 8],\n",
    "    [4, 16, 32],\n",
    "    [32, 16, 4],\n",
    "    [16, 16, 16],\n",
    "    [16, 32, 64],\n",
    "    [64, 32, 16],\n",
    "    [32, 32, 32],\n",
    "    [32, 64, 96],\n",
    "    [96, 64, 32],\n",
    "    [64, 64, 64],\n",
    "    [64, 96, 128],\n",
    "    [128, 96, 64],\n",
    "    [128, 128, 128],\n",
    "    [256, 128, 256],\n",
    "    [128, 256, 128],\n",
    "    [256, 256, 256]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "k=5 # no. of classes\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "model_history = dict()\n",
    "\n",
    "# train different achitectures and optimizers\n",
    "print('Training models with different architectures and optimizers')\n",
    "for layer_dims in [model_arch[1]]:\n",
    "    for reduced_dimension in [32,64,128,256]:\n",
    "        print(f'{reduced_dimension}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}...')\n",
    "        # define model\n",
    "        model = Sequential([\n",
    "            keras.Input(shape=(28, 28, 1)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(layer_dims[0], activation=\"sigmoid\", name=\"layer1\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[1], activation=\"sigmoid\", name=\"layer2\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(layer_dims[2], activation=\"sigmoid\", name=\"layer3\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "            layers.Dense(k, activation=\"softmax\", name=\"output\", \n",
    "                         kernel_initializer=initializer, bias_initializer=initializers.Zeros()),\n",
    "        ])\n",
    "        \n",
    "        # compile model\n",
    "        model.compile(optimizer=optimizer[1], loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # callbacks\n",
    "        my_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=3),\n",
    "            TensorBoard(log_dir=f'./logdir/{optimizer[0]}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}/')\n",
    "        ]\n",
    "        \n",
    "        batch_size=1\n",
    "        if optimizer[0]=='batch':\n",
    "            batch_size = train.cardinality().numpy()\n",
    "        \n",
    "        model_fit = model.fit(train, batch_size=batch_size, epochs=10000, verbose=0, callbacks=my_callbacks, \n",
    "                              validation_split=0.0, validation_data=val, shuffle=True, validation_batch_size=None)\n",
    "        \n",
    "        model_history[f'{optimizer[0]}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}'] = model_fit.history['accuracy']\n",
    "        \n",
    "        hist_metric = 'accuracy'\n",
    "        print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "        model.save(f'models/{optimizer[0]}-{layer_dims[0]}-{layer_dims[1]}-{layer_dims[2]}.tf')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
