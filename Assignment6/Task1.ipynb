{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 18:19:50.589757: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 18:19:50.690769: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-08 18:19:50.693563: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-08 18:19:50.693577: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-08 18:19:51.153053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-08 18:19:51.153097: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-08 18:19:51.153102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = [], []\n",
    "z_train, z_test = [], []\n",
    "chars =  ['a', 'bA', 'dA', 'lA', 'tA']\n",
    "# chars = [\"a\", \"ai\", \"bA\", \"lA\", \"tA\"]\n",
    "\n",
    "def normalize(seq):\n",
    "    min_x, max_x = min(el[0] for el in seq), max(el[0] for el in seq)\n",
    "    min_y, max_y = min(el[1] for el in seq), max(el[1] for el in seq)\n",
    "    for el in seq:\n",
    "        el[0] = (el[0] - min_x)/(max_x-min_x)\n",
    "        el[1] = (el[1] - min_y)/(max_y-min_y)\n",
    "    return seq\n",
    "def read_ip(path):\n",
    "    seq = []\n",
    "    with open(path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        items = contents.split()\n",
    "        for i in range(1,len(items),2):\n",
    "            seq.append([float(items[i]),float(items[i+1])])\n",
    "        # train.append(normalize(seq))\n",
    "    return normalize(seq)\n",
    "\n",
    "for ch in chars:\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/train\"):\n",
    "        train.append(read_ip(\"Group24/Handwriting_Data/\" + str(ch)+\"/train/\" + str(f)))\n",
    "        z_train.append(str(ch))\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/dev\"):\n",
    "        test.append(read_ip(\"Group24/Handwriting_Data/\" + str(ch) + \"/dev/\" + str(f)))\n",
    "        z_test.append(str(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'a':0, 'bA':1, 'dA':2, 'lA':3, 'tA':4}\n",
    "# label_map = {\"a\":0, \"ai\":1, \"bA\":2, \"lA\":3, \"tA\":4}\n",
    "z_train_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_train], 5)\n",
    "z_test_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_test], 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 18:19:52.120488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-08 18:19:52.120510: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-08 18:19:52.120526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (piyush-HP-Spectre-x360-Convertible-13-aw2xxx): /proc/driver/nvidia/version does not exist\n",
      "2023-05-08 18:19:52.120752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1.6109 - accuracy: 0.2222\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.5927 - accuracy: 0.1784\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.5806 - accuracy: 0.1930\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.5702 - accuracy: 0.2018\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.5612 - accuracy: 0.2281\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.5548 - accuracy: 0.2164\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.5476 - accuracy: 0.2281\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 1.5400 - accuracy: 0.2602\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.5322 - accuracy: 0.2719\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.5231 - accuracy: 0.2836\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.5139 - accuracy: 0.2982\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.5041 - accuracy: 0.3158\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 1.4937 - accuracy: 0.3216\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.4819 - accuracy: 0.3509\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.4693 - accuracy: 0.3801\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.4551 - accuracy: 0.3918\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.4399 - accuracy: 0.4006\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.4239 - accuracy: 0.4327\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 1.4066 - accuracy: 0.4708\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.3899 - accuracy: 0.4766\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.3717 - accuracy: 0.4620\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.3525 - accuracy: 0.4620\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.3312 - accuracy: 0.4649\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.3089 - accuracy: 0.4766\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.2845 - accuracy: 0.4912\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.2597 - accuracy: 0.4620\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.2339 - accuracy: 0.4971\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.2099 - accuracy: 0.4825\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.1860 - accuracy: 0.5029\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 1.1563 - accuracy: 0.5029\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.1278 - accuracy: 0.5175\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.1160 - accuracy: 0.5263\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.1031 - accuracy: 0.4942\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.0615 - accuracy: 0.5351\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.0500 - accuracy: 0.5409\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.0416 - accuracy: 0.5439\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.0108 - accuracy: 0.5673\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.9838 - accuracy: 0.5643\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.9888 - accuracy: 0.5614\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.9602 - accuracy: 0.5556\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.9156 - accuracy: 0.5877\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.9535 - accuracy: 0.6053\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.9269 - accuracy: 0.5994\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.8301 - accuracy: 0.6637\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.8689 - accuracy: 0.6170\n",
      "Epoch 46/10000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.8127 - accuracy: 0.6345\n",
      "Epoch 47/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.7951 - accuracy: 0.6316\n",
      "Epoch 48/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.8305 - accuracy: 0.6404\n",
      "Epoch 49/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.7394 - accuracy: 0.6725\n",
      "Epoch 50/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.7024 - accuracy: 0.6901\n",
      "Epoch 51/10000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.7399 - accuracy: 0.6754\n",
      "Epoch 52/10000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.6913 - accuracy: 0.7018\n",
      "Epoch 53/10000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.6748 - accuracy: 0.7135\n",
      "Epoch 54/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.6780 - accuracy: 0.6930\n",
      "Epoch 55/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.6496 - accuracy: 0.7018\n",
      "Epoch 56/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.6492 - accuracy: 0.7398\n",
      "Epoch 57/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6721 - accuracy: 0.6988\n",
      "Epoch 58/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.6112 - accuracy: 0.7485\n",
      "Epoch 59/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.6359 - accuracy: 0.7690\n",
      "Epoch 60/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.5977 - accuracy: 0.7398\n",
      "Epoch 61/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.5916 - accuracy: 0.7251\n",
      "Epoch 62/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.5850 - accuracy: 0.7778\n",
      "Epoch 63/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.5482 - accuracy: 0.7895\n",
      "Epoch 64/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5656 - accuracy: 0.7661\n",
      "Epoch 65/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.5414 - accuracy: 0.7836\n",
      "Epoch 66/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.5322 - accuracy: 0.8012\n",
      "Epoch 67/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.5508 - accuracy: 0.7515\n",
      "Epoch 68/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.5052 - accuracy: 0.8070\n",
      "Epoch 69/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4906 - accuracy: 0.8129\n",
      "Epoch 70/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4980 - accuracy: 0.7953\n",
      "Epoch 71/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4699 - accuracy: 0.8216\n",
      "Epoch 72/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4746 - accuracy: 0.8246\n",
      "Epoch 73/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4914 - accuracy: 0.7865\n",
      "Epoch 74/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4640 - accuracy: 0.8333\n",
      "Epoch 75/10000\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.4381 - accuracy: 0.8450\n",
      "Epoch 76/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4372 - accuracy: 0.8333\n",
      "Epoch 77/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4347 - accuracy: 0.8450\n",
      "Epoch 78/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4253 - accuracy: 0.8216\n",
      "Epoch 79/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.3996 - accuracy: 0.8626\n",
      "Epoch 80/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3895 - accuracy: 0.8684\n",
      "Epoch 81/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3789 - accuracy: 0.8626\n",
      "Epoch 82/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3951 - accuracy: 0.8596\n",
      "Epoch 83/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.4744 - accuracy: 0.8187\n",
      "Epoch 84/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4697 - accuracy: 0.8363\n",
      "Epoch 85/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4385 - accuracy: 0.8421\n",
      "Epoch 86/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3892 - accuracy: 0.8655\n",
      "Epoch 87/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.4703 - accuracy: 0.8129\n",
      "Epoch 88/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4498 - accuracy: 0.8158\n",
      "Epoch 89/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3502 - accuracy: 0.8743\n",
      "Epoch 90/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.4469 - accuracy: 0.8304\n",
      "Epoch 91/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3617 - accuracy: 0.8801\n",
      "Epoch 92/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3781 - accuracy: 0.8626\n",
      "Epoch 93/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3573 - accuracy: 0.8947\n",
      "Epoch 94/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3691 - accuracy: 0.8713\n",
      "Epoch 95/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3826 - accuracy: 0.8421\n",
      "Epoch 96/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.3352 - accuracy: 0.8918\n",
      "Epoch 97/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.4043 - accuracy: 0.8480\n",
      "Epoch 98/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3891 - accuracy: 0.8450\n",
      "Epoch 99/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3181 - accuracy: 0.9006\n",
      "Epoch 100/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4016 - accuracy: 0.8363\n",
      "Epoch 101/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3101 - accuracy: 0.8860\n",
      "Epoch 102/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.3658 - accuracy: 0.8538\n",
      "Epoch 103/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3071 - accuracy: 0.8977\n",
      "Epoch 104/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.3638 - accuracy: 0.8713\n",
      "Epoch 105/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3098 - accuracy: 0.8889\n",
      "Epoch 106/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.3613 - accuracy: 0.8538\n",
      "Epoch 107/10000\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.3129 - accuracy: 0.8918\n",
      "Epoch 108/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3560 - accuracy: 0.8655\n",
      "Epoch 109/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3088 - accuracy: 0.8977\n",
      "Epoch 110/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3408 - accuracy: 0.8713\n",
      "Epoch 111/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.2793 - accuracy: 0.9035\n",
      "Epoch 112/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3587 - accuracy: 0.8713\n",
      "Epoch 113/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3368 - accuracy: 0.8684\n",
      "Epoch 114/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3458 - accuracy: 0.8713\n",
      "Epoch 115/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2907 - accuracy: 0.8977\n",
      "Epoch 116/10000\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.3827 - accuracy: 0.8538\n",
      "Epoch 117/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3439 - accuracy: 0.8801\n",
      "Epoch 118/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4067 - accuracy: 0.8596\n",
      "Epoch 119/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3125 - accuracy: 0.8772\n",
      "Epoch 120/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.4532 - accuracy: 0.8333\n",
      "Epoch 121/10000\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3261 - accuracy: 0.8684\n",
      "epochs: 121, acc: 0.8684210777282715\n",
      "\n",
      "INFO:tensorflow:Assets written to: models/Q1/test.tf/assets\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2666 - accuracy: 0.9100\n",
      "Test loss =  0.26661965250968933\n",
      "Test accuracy =  0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "my_tensor = tf.ragged.constant(train)\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, batch_input_shape=(None, None, 2), return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=True))\n",
    "# model.add(SimpleRNN(64, activation=\"relu\", return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=['accuracy']) \n",
    "# Train the model\n",
    "my_callbacks = [ EarlyStopping(monitor='loss', min_delta=1e-4, patience=10),    TensorBoard(log_dir=f'./logdir/Q1/test')]\n",
    "model_fit = model.fit(my_tensor, z_train_onehot, batch_size=len(train), epochs=10000, verbose=1, validation_split=0.0,callbacks=my_callbacks, shuffle=False, validation_batch_size=None)\n",
    "\n",
    "hist_metric = 'accuracy'\n",
    "print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "model.save(f'models/Q1/test.tf')\n",
    "\n",
    "test_tensor = tf.ragged.constant(test)\n",
    "loss, accuracy = model.evaluate(test_tensor,z_test_onehot)\n",
    "\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
