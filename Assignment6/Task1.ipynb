{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = [], []\n",
    "z_train, z_test = [], []\n",
    "chars =  ['a', 'bA', 'dA', 'lA', 'tA']\n",
    "# chars = [\"a\", \"ai\", \"bA\", \"lA\", \"tA\"]\n",
    "\n",
    "def normalize(seq):\n",
    "    min_x, max_x = min(el[0] for el in seq), max(el[0] for el in seq)\n",
    "    min_y, max_y = min(el[1] for el in seq), max(el[1] for el in seq)\n",
    "    for el in seq:\n",
    "        el[0] = (el[0] - min_x)/(max_x-min_x)\n",
    "        el[1] = (el[1] - min_y)/(max_y-min_y)\n",
    "    return seq\n",
    "def read_ip(path):\n",
    "    seq = []\n",
    "    with open(path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        items = contents.split()\n",
    "        for i in range(1,len(items),2):\n",
    "            seq.append([float(items[i]),float(items[i+1])])\n",
    "        # train.append(normalize(seq))\n",
    "    return normalize(seq)\n",
    "\n",
    "for ch in chars:\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/train\"):\n",
    "        train.append(read_ip(\"Group24/Handwriting_Data/\" + str(ch)+\"/train/\" + str(f)))\n",
    "        z_train.append(str(ch))\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/dev\"):\n",
    "        test.append(read_ip(\"Group24/Handwriting_Data/\" + str(ch) + \"/dev/\" + str(f)))\n",
    "        z_test.append(str(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'a':0, 'bA':1, 'dA':2, 'lA':3, 'tA':4}\n",
    "# label_map = {\"a\":0, \"ai\":1, \"bA\":2, \"lA\":3, \"tA\":4}\n",
    "z_train_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_train], 5)\n",
    "z_test_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_test], 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = tf.ragged.constant(train)\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, batch_input_shape=(None, None, 2), return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=True))\n",
    "# model.add(SimpleRNN(64, activation=\"relu\", return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=['accuracy']) \n",
    "# Train the model\n",
    "my_callbacks = [ EarlyStopping(monitor='loss', min_delta=1e-4, patience=10),    TensorBoard(log_dir=f'./logdir/Q1/test')]\n",
    "model_fit = model.fit(my_tensor, z_train_onehot, batch_size=len(train), epochs=10000, verbose=1, validation_split=0.0,callbacks=my_callbacks, shuffle=False, validation_batch_size=None)\n",
    "\n",
    "hist_metric = 'accuracy'\n",
    "print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "model.save(f'models/Q1/test.tf')\n",
    "\n",
    "test_tensor = tf.ragged.constant(test)\n",
    "loss, accuracy = model.evaluate(test_tensor,z_test_onehot)\n",
    "\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:10:03.834212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-09 17:10:03.834268: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-09 17:10:03.834311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (piyush-HP-Spectre-x360-Convertible-13-aw2xxx): /proc/driver/nvidia/version does not exist\n",
      "2023-05-09 17:10:03.978847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6222 - accuracy: 0.2018\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.6133 - accuracy: 0.2018\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 1.6055 - accuracy: 0.2018\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.5988 - accuracy: 0.2018\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.5929 - accuracy: 0.2018\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.5877 - accuracy: 0.2018\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 1.5831 - accuracy: 0.2018\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.5790 - accuracy: 0.1930\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.5750 - accuracy: 0.2076\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.5711 - accuracy: 0.2251\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.5671 - accuracy: 0.2193\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 1.5629 - accuracy: 0.1959\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.5582 - accuracy: 0.2105\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 1.5531 - accuracy: 0.2105\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 1.5474 - accuracy: 0.2135\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.5411 - accuracy: 0.2222\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.5339 - accuracy: 0.2456\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.5257 - accuracy: 0.2865\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.5161 - accuracy: 0.3187\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.5047 - accuracy: 0.3626\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.4910 - accuracy: 0.4035\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.4741 - accuracy: 0.4152\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.4532 - accuracy: 0.4415\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 1.4272 - accuracy: 0.4386\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.3951 - accuracy: 0.4708\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.3570 - accuracy: 0.5117\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.3172 - accuracy: 0.5380\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 1.2925 - accuracy: 0.5029\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.2930 - accuracy: 0.4737\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.2636 - accuracy: 0.4766\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.2355 - accuracy: 0.5263\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.2311 - accuracy: 0.5088\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.2312 - accuracy: 0.5117\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 1.2270 - accuracy: 0.4883\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 1.2153 - accuracy: 0.4883\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.1935 - accuracy: 0.4912\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.1637 - accuracy: 0.5029\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.1560 - accuracy: 0.5585\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.1485 - accuracy: 0.5263\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.1326 - accuracy: 0.5146\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.1139 - accuracy: 0.5556\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.1070 - accuracy: 0.5819\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.0984 - accuracy: 0.5702\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.0869 - accuracy: 0.5702\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.0694 - accuracy: 0.5819\n",
      "Epoch 46/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.0543 - accuracy: 0.5994\n",
      "Epoch 47/10000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 1.0375 - accuracy: 0.6023\n",
      "Epoch 48/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.0185 - accuracy: 0.6111\n",
      "Epoch 49/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.0000 - accuracy: 0.6140\n",
      "Epoch 50/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.9934 - accuracy: 0.6053\n",
      "Epoch 51/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.0099 - accuracy: 0.5556\n",
      "Epoch 52/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.9876 - accuracy: 0.5819\n",
      "Epoch 53/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.9717 - accuracy: 0.5877\n",
      "Epoch 54/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.9448 - accuracy: 0.5848\n",
      "Epoch 55/10000\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.9312 - accuracy: 0.6053\n",
      "Epoch 56/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.9194 - accuracy: 0.6053\n",
      "Epoch 57/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.8998 - accuracy: 0.6199\n",
      "Epoch 58/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.8939 - accuracy: 0.6608\n",
      "Epoch 59/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.8535 - accuracy: 0.6754\n",
      "Epoch 60/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8556 - accuracy: 0.6520\n",
      "Epoch 61/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.8300 - accuracy: 0.6813\n",
      "Epoch 62/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.8285 - accuracy: 0.6959\n",
      "Epoch 63/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.7968 - accuracy: 0.6871\n",
      "Epoch 64/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.7905 - accuracy: 0.6784\n",
      "Epoch 65/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.7697 - accuracy: 0.6959\n",
      "Epoch 66/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.7629 - accuracy: 0.7281\n",
      "Epoch 67/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.7443 - accuracy: 0.7164\n",
      "Epoch 68/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.7362 - accuracy: 0.7222\n",
      "Epoch 69/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.7127 - accuracy: 0.7690\n",
      "Epoch 70/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.7053 - accuracy: 0.7544\n",
      "Epoch 71/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6945 - accuracy: 0.7661\n",
      "Epoch 72/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6668 - accuracy: 0.7924\n",
      "Epoch 73/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.6594 - accuracy: 0.7749\n",
      "Epoch 74/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6313 - accuracy: 0.8041\n",
      "Epoch 75/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6204 - accuracy: 0.8129\n",
      "Epoch 76/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6094 - accuracy: 0.7924\n",
      "Epoch 77/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.5819 - accuracy: 0.8041\n",
      "Epoch 78/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5790 - accuracy: 0.8158\n",
      "Epoch 79/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.5369 - accuracy: 0.8392\n",
      "Epoch 80/10000\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.5627 - accuracy: 0.8070\n",
      "Epoch 81/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5026 - accuracy: 0.8509\n",
      "Epoch 82/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.5238 - accuracy: 0.8480\n",
      "Epoch 83/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4953 - accuracy: 0.8626\n",
      "Epoch 84/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5021 - accuracy: 0.8480\n",
      "Epoch 85/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4560 - accuracy: 0.8713\n",
      "Epoch 86/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4525 - accuracy: 0.8801\n",
      "Epoch 87/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4141 - accuracy: 0.8977\n",
      "Epoch 88/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4768 - accuracy: 0.8713\n",
      "Epoch 89/10000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.4626 - accuracy: 0.8655\n",
      "Epoch 90/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4241 - accuracy: 0.8918\n",
      "Epoch 91/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4630 - accuracy: 0.8801\n",
      "Epoch 92/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4271 - accuracy: 0.9064\n",
      "Epoch 93/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4299 - accuracy: 0.8860\n",
      "Epoch 94/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4173 - accuracy: 0.9064\n",
      "Epoch 95/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4052 - accuracy: 0.9123\n",
      "Epoch 96/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4054 - accuracy: 0.9094\n",
      "Epoch 97/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3796 - accuracy: 0.9064\n",
      "Epoch 98/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3402 - accuracy: 0.9152\n",
      "Epoch 99/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.4616 - accuracy: 0.8655\n",
      "Epoch 100/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4248 - accuracy: 0.8684\n",
      "Epoch 101/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4297 - accuracy: 0.8480\n",
      "Epoch 102/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3923 - accuracy: 0.8860\n",
      "Epoch 103/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3971 - accuracy: 0.8801\n",
      "Epoch 104/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4240 - accuracy: 0.8772\n",
      "Epoch 105/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4089 - accuracy: 0.8830\n",
      "Epoch 106/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3687 - accuracy: 0.8947\n",
      "Epoch 107/10000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.3931 - accuracy: 0.8801\n",
      "Epoch 108/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.3818 - accuracy: 0.8977\n",
      "epochs: 108, acc: 0.8976607918739319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Q1/LSTM.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Q1/LSTM.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.2606 - accuracy: 0.9300\n",
      "Test loss =  0.26064276695251465\n",
      "Test accuracy =  0.9300000071525574\n"
     ]
    }
   ],
   "source": [
    "train_tensor = tf.ragged.constant(train)\n",
    "test_tensor = tf.ragged.constant(test)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, batch_input_shape=(None,None,2)),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=['accuracy']) \n",
    "\n",
    "my_callbacks = [ EarlyStopping(monitor='loss', min_delta=1e-4, patience=10),    TensorBoard(log_dir=f'./logdir/Q1/LSTM')]\n",
    "model_fit = model.fit(train_tensor, z_train_onehot, batch_size=len(train), epochs=10000, verbose=1, validation_split=0.0,callbacks=my_callbacks, shuffle=False, validation_batch_size=None)\n",
    "\n",
    "hist_metric = 'accuracy'\n",
    "print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "model.save(f'models/Q1/LSTM.tf')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_tensor,z_test_onehot)\n",
    "\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
