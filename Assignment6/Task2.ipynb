{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 23:12:55.485159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 23:12:55.645487: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-08 23:12:55.648601: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-08 23:12:55.648622: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-08 23:12:56.209453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-08 23:12:56.209508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-08 23:12:56.209514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = [], []\n",
    "z_train, z_test = [], []\n",
    "# chars =  [\"bhA\", \"ba\", \"tA\", \"kaa\", \"tI\"]\n",
    "chars = [\"hai\", \"ka\", \"rI\", \"sa\", \"pa\"]\n",
    "\n",
    "def read_ip(path):\n",
    "    seq = []\n",
    "    with open(path, 'r') as file:\n",
    "        # Iterate over the lines in the file\n",
    "        for line in file:\n",
    "            data = [float(freq) for freq in line.split()]\n",
    "            seq.append(data)\n",
    "    return seq\n",
    "\n",
    "for ch in chars:\n",
    "    for f in os.listdir(\"Group24/CV_Data/\" + str(ch)+\"/Train\"):\n",
    "        train.append(read_ip(\"Group24/CV_Data/\" + str(ch)+\"/Train/\" + str(f)))\n",
    "        z_train.append(str(ch))\n",
    "    for f in os.listdir(\"Group24/CV_Data/\" + str(ch)+\"/Test\"):\n",
    "        test.append(read_ip(\"Group24/CV_Data/\" + str(ch) + \"/Test/\" + str(f)))\n",
    "        z_test.append(str(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, z_train, z_val = train_test_split(train, z_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {\"bhA\":0, \"ba\":1, \"tA\":2, \"kaa\":3, \"tI\":4}\n",
    "label_map = {\"hai\":0, \"ka\":1, \"rI\":2, \"sa\":3, \"pa\":4}\n",
    "z_train_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_train], 5)\n",
    "z_val_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_val], 5)\n",
    "z_test_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_test], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 23:13:00.469346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-08 23:13:00.469369: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-08 23:13:00.469383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (piyush-HP-Spectre-x360-Convertible-13-aw2xxx): /proc/driver/nvidia/version does not exist\n",
      "2023-05-08 23:13:00.469620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_tensor = tf.ragged.constant(train)\n",
    "test_tensor = tf.ragged.constant(test)\n",
    "val_tensor = tf.ragged.constant(val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/1 [==============================] - 4s 4s/step - loss: 2.1459 - accuracy: 0.2504 - val_loss: 1.5433 - val_accuracy: 0.3409\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 1s 814ms/step - loss: 1.5726 - accuracy: 0.3385 - val_loss: 1.4614 - val_accuracy: 0.4062\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 1.4832 - accuracy: 0.3947 - val_loss: 1.3836 - val_accuracy: 0.4517\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 1.3981 - accuracy: 0.4403 - val_loss: 1.3106 - val_accuracy: 0.5000\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 1.3281 - accuracy: 0.4730 - val_loss: 1.2421 - val_accuracy: 0.5398\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 1s 815ms/step - loss: 1.2628 - accuracy: 0.4929 - val_loss: 1.1758 - val_accuracy: 0.5767\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 1s 767ms/step - loss: 1.1963 - accuracy: 0.5235 - val_loss: 1.1229 - val_accuracy: 0.5767\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 1.1370 - accuracy: 0.5448 - val_loss: 1.0743 - val_accuracy: 0.6080\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 1s 776ms/step - loss: 1.0807 - accuracy: 0.5782 - val_loss: 1.0310 - val_accuracy: 0.6165\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 1s 774ms/step - loss: 1.0297 - accuracy: 0.6017 - val_loss: 0.9902 - val_accuracy: 0.6165\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.9870 - accuracy: 0.6152 - val_loss: 0.9463 - val_accuracy: 0.6335\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 1s 776ms/step - loss: 0.9503 - accuracy: 0.6330 - val_loss: 0.9085 - val_accuracy: 0.6307\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 1s 840ms/step - loss: 0.9204 - accuracy: 0.6444 - val_loss: 0.8719 - val_accuracy: 0.6449\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 0.8861 - accuracy: 0.6572 - val_loss: 0.8397 - val_accuracy: 0.6619\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 1s 893ms/step - loss: 0.8545 - accuracy: 0.6714 - val_loss: 0.8087 - val_accuracy: 0.6676\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8200 - accuracy: 0.6792 - val_loss: 0.7836 - val_accuracy: 0.6903\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7869 - accuracy: 0.6927 - val_loss: 0.7645 - val_accuracy: 0.6932\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 1s 929ms/step - loss: 0.7578 - accuracy: 0.6977 - val_loss: 0.7435 - val_accuracy: 0.6903\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 0.7279 - accuracy: 0.7112 - val_loss: 0.7294 - val_accuracy: 0.6960\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 0.6980 - accuracy: 0.7240 - val_loss: 0.7015 - val_accuracy: 0.7074\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 1s 944ms/step - loss: 0.6676 - accuracy: 0.7326 - val_loss: 0.6771 - val_accuracy: 0.7216\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 1s 986ms/step - loss: 0.6411 - accuracy: 0.7432 - val_loss: 0.6566 - val_accuracy: 0.7273\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6163 - accuracy: 0.7525 - val_loss: 0.6587 - val_accuracy: 0.7188\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5950 - accuracy: 0.7582 - val_loss: 0.6475 - val_accuracy: 0.7386\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5837 - accuracy: 0.7639 - val_loss: 0.6807 - val_accuracy: 0.7301\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5868 - accuracy: 0.7617 - val_loss: 0.6341 - val_accuracy: 0.7557\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 1s 954ms/step - loss: 0.5680 - accuracy: 0.7731 - val_loss: 0.6174 - val_accuracy: 0.7642\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 1s 932ms/step - loss: 0.5293 - accuracy: 0.7888 - val_loss: 0.6312 - val_accuracy: 0.7557\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.5243 - accuracy: 0.7909 - val_loss: 0.6089 - val_accuracy: 0.7585\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 1s 962ms/step - loss: 0.4963 - accuracy: 0.7973 - val_loss: 0.6068 - val_accuracy: 0.7585\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 1s 973ms/step - loss: 0.4871 - accuracy: 0.8065 - val_loss: 0.6072 - val_accuracy: 0.7642\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4659 - accuracy: 0.8115 - val_loss: 0.6223 - val_accuracy: 0.7585\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4498 - accuracy: 0.8250 - val_loss: 0.6100 - val_accuracy: 0.7642\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4330 - accuracy: 0.8343 - val_loss: 0.6006 - val_accuracy: 0.7727\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4156 - accuracy: 0.8428 - val_loss: 0.6067 - val_accuracy: 0.7699\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4043 - accuracy: 0.8450 - val_loss: 0.6084 - val_accuracy: 0.7727\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.3897 - accuracy: 0.8570 - val_loss: 0.6169 - val_accuracy: 0.7585\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 1s 964ms/step - loss: 0.3751 - accuracy: 0.8592 - val_loss: 0.6211 - val_accuracy: 0.7756\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 1s 953ms/step - loss: 0.3644 - accuracy: 0.8521 - val_loss: 0.6080 - val_accuracy: 0.7784\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.3497 - accuracy: 0.8677 - val_loss: 0.6085 - val_accuracy: 0.7614\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 1s 942ms/step - loss: 0.3346 - accuracy: 0.8727 - val_loss: 0.6154 - val_accuracy: 0.7642\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3218 - accuracy: 0.8805 - val_loss: 0.6053 - val_accuracy: 0.7784\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3086 - accuracy: 0.8869 - val_loss: 0.6201 - val_accuracy: 0.7756\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2971 - accuracy: 0.8912 - val_loss: 0.6162 - val_accuracy: 0.7784\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2863 - accuracy: 0.8969 - val_loss: 0.6334 - val_accuracy: 0.7756\n",
      "Epoch 46/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2768 - accuracy: 0.8912 - val_loss: 0.6151 - val_accuracy: 0.7699\n",
      "Epoch 47/10000\n",
      "1/1 [==============================] - 1s 949ms/step - loss: 0.2739 - accuracy: 0.9011 - val_loss: 0.6369 - val_accuracy: 0.7699\n",
      "Epoch 48/10000\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 0.2547 - accuracy: 0.9075 - val_loss: 0.6048 - val_accuracy: 0.7869\n",
      "Epoch 49/10000\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.2343 - accuracy: 0.9225 - val_loss: 0.5967 - val_accuracy: 0.7841\n",
      "Epoch 50/10000\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.2227 - accuracy: 0.9275 - val_loss: 0.6154 - val_accuracy: 0.7784\n",
      "Epoch 51/10000\n",
      "1/1 [==============================] - 1s 942ms/step - loss: 0.2215 - accuracy: 0.9189 - val_loss: 0.5992 - val_accuracy: 0.7898\n",
      "Epoch 52/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2112 - accuracy: 0.9303 - val_loss: 0.5965 - val_accuracy: 0.7898\n",
      "Epoch 53/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1892 - accuracy: 0.9395 - val_loss: 0.6222 - val_accuracy: 0.7869\n",
      "Epoch 54/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1891 - accuracy: 0.9367 - val_loss: 0.6022 - val_accuracy: 0.7784\n",
      "Epoch 55/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1869 - accuracy: 0.9374 - val_loss: 0.5870 - val_accuracy: 0.7983\n",
      "Epoch 56/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1593 - accuracy: 0.9523 - val_loss: 0.6213 - val_accuracy: 0.7955\n",
      "Epoch 57/10000\n",
      "1/1 [==============================] - 1s 957ms/step - loss: 0.1672 - accuracy: 0.9417 - val_loss: 0.6223 - val_accuracy: 0.7841\n",
      "Epoch 58/10000\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 0.1727 - accuracy: 0.9431 - val_loss: 0.5853 - val_accuracy: 0.7983\n",
      "Epoch 59/10000\n",
      "1/1 [==============================] - 1s 949ms/step - loss: 0.1324 - accuracy: 0.9595 - val_loss: 0.6854 - val_accuracy: 0.7983\n",
      "Epoch 60/10000\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 0.1661 - accuracy: 0.9367 - val_loss: 0.6618 - val_accuracy: 0.7756\n",
      "Epoch 61/10000\n",
      "1/1 [==============================] - 1s 951ms/step - loss: 0.1626 - accuracy: 0.9445 - val_loss: 0.6002 - val_accuracy: 0.7869\n",
      "Epoch 62/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1222 - accuracy: 0.9623 - val_loss: 0.6665 - val_accuracy: 0.7955\n",
      "Epoch 63/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1372 - accuracy: 0.9495 - val_loss: 0.5879 - val_accuracy: 0.8068\n",
      "Epoch 64/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0979 - accuracy: 0.9765 - val_loss: 0.6125 - val_accuracy: 0.7841\n",
      "Epoch 65/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1075 - accuracy: 0.9730 - val_loss: 0.6065 - val_accuracy: 0.8040\n",
      "Epoch 66/10000\n",
      "1/1 [==============================] - 1s 953ms/step - loss: 0.0896 - accuracy: 0.9808 - val_loss: 0.6520 - val_accuracy: 0.7926\n",
      "Epoch 67/10000\n",
      "1/1 [==============================] - 1s 951ms/step - loss: 0.0940 - accuracy: 0.9758 - val_loss: 0.6108 - val_accuracy: 0.8125\n",
      "Epoch 68/10000\n",
      "1/1 [==============================] - 1s 938ms/step - loss: 0.0744 - accuracy: 0.9851 - val_loss: 0.6247 - val_accuracy: 0.8153\n",
      "Epoch 69/10000\n",
      "1/1 [==============================] - 1s 956ms/step - loss: 0.0806 - accuracy: 0.9808 - val_loss: 0.6796 - val_accuracy: 0.8040\n",
      "Epoch 70/10000\n",
      "1/1 [==============================] - 1s 945ms/step - loss: 0.0719 - accuracy: 0.9858 - val_loss: 0.6151 - val_accuracy: 0.8210\n",
      "Epoch 71/10000\n",
      "1/1 [==============================] - 1s 925ms/step - loss: 0.0618 - accuracy: 0.9879 - val_loss: 0.6105 - val_accuracy: 0.8324\n",
      "Epoch 72/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0575 - accuracy: 0.9879 - val_loss: 0.6106 - val_accuracy: 0.8324\n",
      "Epoch 73/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0513 - accuracy: 0.9908 - val_loss: 0.6266 - val_accuracy: 0.8352\n",
      "Epoch 74/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0466 - accuracy: 0.9936 - val_loss: 0.6878 - val_accuracy: 0.8210\n",
      "Epoch 75/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0445 - accuracy: 0.9929 - val_loss: 0.6621 - val_accuracy: 0.8352\n",
      "Epoch 76/10000\n",
      "1/1 [==============================] - 1s 987ms/step - loss: 0.0392 - accuracy: 0.9950 - val_loss: 0.6440 - val_accuracy: 0.8352\n",
      "Epoch 77/10000\n",
      "1/1 [==============================] - 1s 949ms/step - loss: 0.0361 - accuracy: 0.9957 - val_loss: 0.6339 - val_accuracy: 0.8381\n",
      "Epoch 78/10000\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 0.0322 - accuracy: 0.9964 - val_loss: 0.6355 - val_accuracy: 0.8438\n",
      "epochs: 78, acc: 0.9964438080787659\n",
      "\n",
      "INFO:tensorflow:Assets written to: models/Q1/test.tf/assets\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 0.7294 - accuracy: 0.8231\n",
      "Test loss =  0.7294228076934814\n",
      "Test accuracy =  0.8231292366981506\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(256, batch_input_shape=(None, None, 39), return_sequences=True))\n",
    "model.add(SimpleRNN(128, activation=\"relu\", return_sequences=True))\n",
    "model.add(SimpleRNN(64, activation=\"relu\", return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=['accuracy']) \n",
    "# Train the model\n",
    "my_callbacks = [ EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=20),    TensorBoard(log_dir=f'./logdir/Q1/test')]\n",
    "model_fit = model.fit(train_tensor, z_train_onehot, validation_data=(val_tensor, z_val_onehot), batch_size=len(train), epochs=10000, verbose=1, validation_split=0.0,callbacks=my_callbacks, shuffle=False, validation_batch_size=None)\n",
    "\n",
    "hist_metric = 'accuracy'\n",
    "print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "model.save(f'models/Q1/test.tf')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_tensor,z_test_onehot)\n",
    "\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
