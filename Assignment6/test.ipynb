{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = [], []\n",
    "z_train, z_test = [], []\n",
    "# chars =  ['a', 'bA', 'dA', 'lA', 'tA']\n",
    "chars = [\"a\", \"ai\", \"bA\", \"lA\", \"tA\"]\n",
    "\n",
    "def normalize(seq):\n",
    "    min_x, max_x = min(el[0] for el in seq), max(el[0] for el in seq)\n",
    "    min_y, max_y = min(el[1] for el in seq), max(el[1] for el in seq)\n",
    "    for el in seq:\n",
    "        el[0] = (el[0] - min_x)/(max_x-min_x)\n",
    "        el[1] = (el[1] - min_y)/(max_y-min_y)\n",
    "    return seq\n",
    "def read_ip(path):\n",
    "    seq = []\n",
    "    with open(path, 'r') as file:\n",
    "        contents = file.read()\n",
    "        items = contents.split()\n",
    "        for i in range(1,len(items),2):\n",
    "            seq.append([float(items[i]),float(items[i+1])])\n",
    "        train.append(normalize(seq))\n",
    "    return seq\n",
    "\n",
    "for ch in chars:\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/train\"):\n",
    "        with open(\"Group24/Handwriting_Data/\" + str(ch)+\"/train/\" + str(f), 'r') as file:\n",
    "            contents = file.read()\n",
    "            items = contents.split()\n",
    "            seq = []\n",
    "            for i in range(1,len(items),2):\n",
    "                seq.append([float(items[i]),float(items[i+1])])\n",
    "            train.append(normalize(seq))\n",
    "        z_train.append(str(ch))\n",
    "    for f in os.listdir(\"Group24/Handwriting_Data/\" + str(ch)+\"/dev\"):\n",
    "        with open(\"Group24/Handwriting_Data/\" + str(ch)+\"/dev/\" + str(f), 'r') as file:\n",
    "            contents = file.read()\n",
    "            items = contents.split()\n",
    "            seq = []\n",
    "            for i in range(1,len(items),2):\n",
    "                seq.append([float(items[i]),float(items[i+1])])\n",
    "            test.append(normalize(seq))\n",
    "        z_test.append(str(ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {'a':0, 'bA':1, 'dA':2, 'lA':3, 'tA':4}\n",
    "label_map = {\"a\":0, \"ai\":1, \"bA\":2, \"lA\":3, \"tA\":4}\n",
    "z_train_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_train], 5)\n",
    "z_test_onehot = tf.keras.utils.to_categorical([label_map[x] for x in z_test], 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7999 - accuracy: 0.3090\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.7428 - accuracy: 0.3265\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.6994 - accuracy: 0.3499\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.6682 - accuracy: 0.3499\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.6452 - accuracy: 0.3819\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.6283 - accuracy: 0.3848\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.6140 - accuracy: 0.3848\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.6009 - accuracy: 0.3819\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.5899 - accuracy: 0.3907\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5808 - accuracy: 0.4082\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.5728 - accuracy: 0.4140\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.5650 - accuracy: 0.4257\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.5571 - accuracy: 0.4373\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.5481 - accuracy: 0.4490\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.5368 - accuracy: 0.4665\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.5240 - accuracy: 0.4694\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.5102 - accuracy: 0.4519\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.4958 - accuracy: 0.4636\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4806 - accuracy: 0.4577\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.4637 - accuracy: 0.4636\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.4441 - accuracy: 0.4694\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.4236 - accuracy: 0.4810\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.4014 - accuracy: 0.4869\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.3777 - accuracy: 0.4985\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.3576 - accuracy: 0.5044\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.3343 - accuracy: 0.5131\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3188 - accuracy: 0.5190\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.2977 - accuracy: 0.5219\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.2805 - accuracy: 0.5219\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2645 - accuracy: 0.5190\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.2443 - accuracy: 0.5306\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.2370 - accuracy: 0.5831\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.2104 - accuracy: 0.5277\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1893 - accuracy: 0.5306\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.1822 - accuracy: 0.5190\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 1.1340 - accuracy: 0.5860\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 1.1270 - accuracy: 0.5860\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.0843 - accuracy: 0.5569\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0847 - accuracy: 0.5306\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0524 - accuracy: 0.5394\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0167 - accuracy: 0.5714\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0052 - accuracy: 0.5889\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.9859 - accuracy: 0.5977\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.9491 - accuracy: 0.6006\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9681 - accuracy: 0.6006"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m my_callbacks \u001b[39m=\u001b[39m [ EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m),    TensorBoard(log_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./logdir/Q1/test\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m---> 15\u001b[0m model_fit \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(my_tensor, z_train_onehot, batch_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train), epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m,callbacks\u001b[39m=\u001b[39;49mmy_callbacks, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, validation_batch_size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m hist_metric \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepochs: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(model_fit\u001b[39m.\u001b[39mhistory[hist_metric])\u001b[39m}\u001b[39;00m\u001b[39m, acc: \u001b[39m\u001b[39m{\u001b[39;00mmodel_fit\u001b[39m.\u001b[39mhistory[hist_metric][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1712\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m     }\n\u001b[1;32m   1710\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1712\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1713\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    453\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 454\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:2752\u001b[0m, in \u001b[0;36mTensorBoard.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2750\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2751\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs metrics and histogram summaries at epoch end.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2752\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_epoch_metrics(epoch, logs)\n\u001b[1;32m   2754\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram_freq \u001b[39mand\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2755\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_weights(epoch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:2811\u001b[0m, in \u001b[0;36mTensorBoard._log_epoch_metrics\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mrecord_if(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   2810\u001b[0m     \u001b[39mif\u001b[39;00m train_logs:\n\u001b[0;32m-> 2811\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m   2812\u001b[0m             \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m train_logs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   2813\u001b[0m                 tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name, value, step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:91\u001b[0m, in \u001b[0;36m_SummaryContextManager.__exit__\u001b[0;34m(self, *exc)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mexc):\n\u001b[1;32m     89\u001b[0m   \u001b[39m# Flushes the summary writer in eager mode or in graph functions, but\u001b[39;00m\n\u001b[1;32m     90\u001b[0m   \u001b[39m# not in legacy graph mode (you're on your own there).\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m   _summary_state\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mflush()\n\u001b[1;32m     92\u001b[0m   _summary_state\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_old_writer\n\u001b[1;32m     93\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/summary_ops_v2.py:347\u001b[0m, in \u001b[0;36m_ResourceSummaryWriter.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 347\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_summary_ops\u001b[39m.\u001b[39;49mflush_summary_writer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resource)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_summary_ops.py:194\u001b[0m, in \u001b[0;36mflush_summary_writer\u001b[0;34m(writer, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m    193\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m    195\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mFlushSummaryWriter\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, writer)\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m    197\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_tensor = tf.ragged.constant(train)\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, batch_input_shape=(None, None, 2), return_sequences=True))\n",
    "# model.add(SimpleRNN(32, activation=\"relu\", return_sequences=True))\n",
    "# model.add(SimpleRNN(64, activation=\"relu\", return_sequences=True))\n",
    "model.add(SimpleRNN(32, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam_optimizer, metrics=['accuracy']) \n",
    "# Train the model\n",
    "my_callbacks = [ EarlyStopping(monitor='loss', min_delta=1e-4, patience=10),    TensorBoard(log_dir=f'./logdir/Q1/test')]\n",
    "model_fit = model.fit(my_tensor, z_train_onehot, batch_size=len(train), epochs=10000, verbose=1, validation_split=0.0,callbacks=my_callbacks, shuffle=False, validation_batch_size=None)\n",
    "\n",
    "hist_metric = 'accuracy'\n",
    "print(f'epochs: {len(model_fit.history[hist_metric])}, acc: {model_fit.history[hist_metric][-1]}\\n')\n",
    "model.save(f'models/Q1/test.tf')\n",
    "\n",
    "test_tensor = tf.ragged.constant(test)\n",
    "loss, accuracy = model.evaluate(test_tensor,z_test_onehot)\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Q1/best_model_100_64_64_50.tf/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f'models/Q1/best_model_100_64_64_50.tf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastest : 32(rnn),32(rnn),32(dense),5(dense)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
